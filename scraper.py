# scraper.py
import asyncio
from playwright.async_api import async_playwright
import re
import aiosqlite
from db import init_db, DATABASE

TARGET_URL = 'https://x.com/'

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å—á–µ—Ç—á–∏–∫–∏
total_found = 0
total_saved = 0

async def extract_post_data(post, db):
    global total_found, total_saved
    try:
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –ø–æ—Å—Ç–∞ –∏–∑ —Å—Å—ã–ª–∫–∏
        post_link = await post.query_selector('a[href*="/status/"]')
        if not post_link:
            return

        href = await post_link.get_attribute('href')
        post_id_match = re.search(r'/status/(\d+)', href)
        if not post_id_match:
            return
        post_id = post_id_match.group(1)

        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤
        total_found += 1

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø–æ—Å—Ç–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        async with db.execute('SELECT id FROM posts WHERE id = ?', (post_id,)) as cursor:
            row = await cursor.fetchone()
            if row:
                # –ü–æ—Å—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                return

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        username_container = await post.query_selector('[data-testid="User-Name"]')
        username = ''
        if username_container:
            username_link = await username_container.query_selector('a[href^="/"]')
            if username_link:
                username_href = await username_link.get_attribute('href')
                if username_href:
                    username = username_href.strip('/').split('/')[0]

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã
        time_element = await post.query_selector('time')
        datetime = ''
        if time_element:
            datetime = await time_element.get_attribute('datetime')

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_elements = await post.query_selector_all('[data-testid="tweetPhoto"] img')
        images = []
        for img in image_elements:
            src = await img.get_attribute('src')
            if src:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞—Å—Ç–∏ URL –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è
                url_match = re.search(r'(https:\/\/pbs\.twimg\.com\/media\/[^?]+\?format=jpg)', src)
                if url_match:
                    base_url = url_match.group(1)
                    high_res_url = base_url + '&name=4096x4096'
                    images.append(high_res_url)

        if not images:
            # –ï—Å–ª–∏ –≤ –ø–æ—Å—Ç–µ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ
            return

        # –í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        await db.execute('INSERT INTO posts (id, username, datetime) VALUES (?, ?, ?)',
                         (post_id, username, datetime))
        for index, image_url in enumerate(images, start=1):
            await db.execute('INSERT INTO images (post_id, image_number, image_url) VALUES (?, ?, ?)',
                             (post_id, index, image_url))
        await db.commit()
        total_saved += 1
        print(f'‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω –ø–æ—Å—Ç {post_id}')
        print(f'   –ê–≤—Ç–æ—Ä: {username}')
        print(f'   –î–∞—Ç–∞: {datetime}')
        print(f'   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}')
        print(f'   –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {total_found}, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {total_saved}\n')

    except Exception as e:
        print(f'‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–∞: {e}')

async def scan_posts(page, db):
    # –°–µ–ª–µ–∫—Ç–æ—Ä –ø–æ—Å—Ç–∞, –≤–æ–∑–º–æ–∂–Ω–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞
    posts = await page.query_selector_all('[data-testid="tweet"]')
    for post in posts:
        await extract_post_data(post, db)

async def main():
    await init_db()
    async with aiosqlite.connect(DATABASE) as db:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=False)  # headless=True –¥–ª—è –±–µ–∑–≥–æ–ª–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
            page = await browser.new_page()
            await page.goto(TARGET_URL, wait_until='networkidle')

            # –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
            await scan_posts(page, db)

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –ø—Ä–∏ —Å–∫—Ä–æ–ª–ª–∏–Ω–≥–µ
            print('üöÄ –°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—â–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞—á–Ω–∏—Ç–µ —Å–∫—Ä–æ–ª–ª–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É.')

            async def periodic_scan():
                while True:
                    await asyncio.sleep(1)
                    await scan_posts(page, db)
                    print(f'üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {total_found}, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {total_saved}\n')

            # –ó–∞–ø—É—Å–∫ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
            scan_task = asyncio.create_task(periodic_scan())

            # –ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞ –ø–æ—Å–ª–µ 1 —á–∞—Å–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            async def close_after_timeout():
                await asyncio.sleep(3600)  # 1 —á–∞—Å
                scan_task.cancel()
                await browser.close()
                await db.close()
                print('üîö –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–µ–Ω.')

            timeout_task = asyncio.create_task(close_after_timeout())

            try:
                await asyncio.gather(scan_task, timeout_task)
            except asyncio.CancelledError:
                pass

if __name__ == '__main__':
    asyncio.run(main())
